{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4483f78d",
   "metadata": {},
   "source": [
    "# Building a RAG System With Google's Gemma, Hugging Face and MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da73485",
   "metadata": {},
   "source": [
    "## Step 1: Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170c6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets pandas pymongo sentence_transformers\n",
    "# !pip install -U transformers\n",
    "# Install below if using GPU\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9274e56",
   "metadata": {},
   "source": [
    "## Step 2: data sourcing and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a0d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "# https://huggingface.co/datasets/MongoDB/embedded_movies\n",
    "dataset = load_dataset(\"MongoDB/embedded_movies\")\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "dataset_df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf28f265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of missing values in each column after removal:\n",
      "plot                    0\n",
      "runtime                14\n",
      "genres                  0\n",
      "fullplot                0\n",
      "directors              12\n",
      "writers                13\n",
      "countries               0\n",
      "poster                 78\n",
      "languages               1\n",
      "cast                    1\n",
      "title                   0\n",
      "num_mflix_comments      0\n",
      "rated                 279\n",
      "imdb                    0\n",
      "awards                  0\n",
      "type                    0\n",
      "metacritic            893\n",
      "plot_embedding          1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove data point where plot column is missing\n",
    "dataset_df = dataset_df.dropna(subset=['fullplot'])\n",
    "print(\"\\nNumber of missing values in each column after removal:\")\n",
    "print(dataset_df.isnull().sum())\n",
    "\n",
    "# Remove the plot_embedding from each data point in the dataset as we are going to create new embeddings with an open-source embedding model from Hugging Face: gte-large\n",
    "dataset_df = dataset_df.drop(columns=['plot_embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d109d12",
   "metadata": {},
   "source": [
    "## Step 3: generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb528540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\LAP TRINH PYTHON\\NhapMonDuLieuLon\\Build_RAG_System_with_Gemini_and_MongoDB\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Initializing Sentence Transformer model...\n",
      "Sentence Transformer model initialized successfully.\n",
      "Generating embeddings for 'fullplot' column...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"Initializing Sentence Transformer model...\")\n",
    "try:\n",
    "    # https://huggingface.co/thenlper/gte-large\n",
    "    embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
    "    print(\"Sentence Transformer model initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Sentence Transformer: {e}\")\n",
    "    print(\"Please ensure PyTorch, transformers, and sentence-transformers versions are compatible.\")\n",
    "    # Có thể thêm exit() ở đây nếu không khởi tạo được model\n",
    "    exit()\n",
    "\n",
    "\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Generates embedding for a given text using the loaded Sentence Transformer model.\"\"\"\n",
    "    # Kiểm tra input đầu vào cẩn thận hơn\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        # print(f\"Skipping embedding for empty or invalid text: {text}\")\n",
    "        return [] # Trả về list rỗng chuẩn dimension là tốt nhất, nhưng phức tạp hơn. Trả về list rỗng đơn giản hơn.\n",
    "\n",
    "    try:\n",
    "        embedding = embedding_model.encode(text)\n",
    "        return embedding.tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding text: '{text[:50]}...' - Error: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# --- Tạo embedding cho cột 'fullplot' ---\n",
    "print(\"Generating embeddings for 'fullplot' column...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Sử dụng apply để tạo embedding\n",
    "# Lưu ý: .apply có thể chậm với dataset lớn.\n",
    "# Cân nhắc dùng embedding_model.encode với cả list text để nhanh hơn nếu dataset lớn.\n",
    "# Ví dụ tối ưu hóa (Optional - chỉ làm nếu apply quá chậm):\n",
    "# texts_to_encode = dataset_df[\"fullplot\"].tolist()\n",
    "# batch_size = 32 # Tùy chỉnh batch size phù hợp với RAM/VRAM\n",
    "# all_embeddings = embedding_model.encode(texts_to_encode, batch_size=batch_size, show_progress_bar=True)\n",
    "# dataset_df[\"embedding\"] = all_embeddings.tolist() # Chuyển numpy array thành list\n",
    "\n",
    "# Cách dùng apply (như code gốc):\n",
    "dataset_df[\"embedding\"] = dataset_df[\"fullplot\"].apply(get_embedding)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Embedding generation completed in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# Kiểm tra xem có bao nhiêu embedding bị lỗi (trả về list rỗng)\n",
    "num_empty_embeddings = dataset_df[\"embedding\"].apply(lambda x: len(x) == 0).sum()\n",
    "if num_empty_embeddings > 0:\n",
    "    print(f\"Warning: {num_empty_embeddings} rows failed to generate embeddings.\")\n",
    "\n",
    "# Hiển thị head để kiểm tra cột embedding mới\n",
    "print(\"\\nDataFrame head after generating embeddings:\")\n",
    "print(dataset_df.head())\n",
    "\n",
    "# Kiểm tra dimension của một embedding hợp lệ (nếu có)\n",
    "first_valid_embedding = dataset_df[dataset_df[\"embedding\"].apply(lambda x: len(x) > 0)][\"embedding\"].iloc[0]\n",
    "if first_valid_embedding:\n",
    "    print(f\"\\nDimension of the first valid embedding: {len(first_valid_embedding)}\") # Phải là 1024 cho gte-large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbca162",
   "metadata": {},
   "source": [
    "## Step 4: database setup and connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ca5062",
   "metadata": {},
   "source": [
    "## Step 5: create vector search index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3136775",
   "metadata": {},
   "source": [
    "## Step 6: establish data connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c3675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n",
      "Selected database 'movies' and collection 'movie_collection_2'.\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def get_mongo_client(mongo_uri):\n",
    "  if not mongo_uri:\n",
    "      print(\"MongoDB URI is missing.\")\n",
    "      return None\n",
    "  try:\n",
    "      client = pymongo.MongoClient(mongo_uri)\n",
    "      client.admin.command(\"ping\")\n",
    "      print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "      return client\n",
    "  except pymongo.errors.ConfigurationError as e:\n",
    "      print(f\"Configuration error: {e}\")\n",
    "      return None\n",
    "  except pymongo.errors.ConnectionFailure as e:\n",
    "      print(f\"Connection failed: {e}\")\n",
    "      return None\n",
    "  except Exception as e:\n",
    "      print(f\"An unexpected error occurred during connection: {e}\")\n",
    "      return None\n",
    "\n",
    "\n",
    "mongo_uri = os.getenv(\"MONGO_URI\")\n",
    "if not mongo_uri:\n",
    "  print(\"MONGO_URI not found. Please ensure it is set in your .env file.\")\n",
    "  exit()\n",
    "mongo_client = get_mongo_client(mongo_uri)\n",
    "if mongo_client:\n",
    "  db_name = \"movies\"\n",
    "  collection_name = \"movie_collection_2\"\n",
    "  db = mongo_client[db_name]\n",
    "  collection = db[collection_name]\n",
    "  print(f\"Selected database '{db_name}' and collection '{collection_name}'.\")\n",
    "else:\n",
    "  print(\n",
    "      \"Could not establish MongoDB connection. Please check your MONGO_URI and network settings/IP whitelist.\"\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c00f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion into MongoDB completed\n"
     ]
    }
   ],
   "source": [
    "# documents = dataset_df.to_dict('records')\n",
    "# collection.insert_many(documents)\n",
    "# print(\"Data ingestion into MongoDB completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74bf5db",
   "metadata": {},
   "source": [
    "## Step 7: Perform Vector Search on User Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e3e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(user_query, collection):\n",
    "    \"\"\"\n",
    "    Perform a vector search in the MongoDB collection based on the user query.\n",
    "\n",
    "    Args:\n",
    "    user_query (str): The user's query string.\n",
    "    collection (MongoCollection): The MongoDB collection to search.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of matching documents.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate embedding for the user query\n",
    "    query_embedding = get_embedding(user_query)\n",
    "\n",
    "    if query_embedding is None:\n",
    "        return \"Invalid query or embedding generation failed.\"\n",
    "\n",
    "    # Define the vector search pipeline\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"index\": \"vector_index\",\n",
    "                \"queryVector\": query_embedding,\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 150,  # Number of candidate matches to consider\n",
    "                \"limit\": 4,  # Return top 4 matches\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"_id\": 0,  # Exclude the _id field\n",
    "                \"fullplot\": 1,  # Include the plot field\n",
    "                \"title\": 1,  # Include the title field\n",
    "                \"genres\": 1,  # Include the genres field\n",
    "                \"score\": {\"$meta\": \"vectorSearchScore\"},  # Include the search score\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Execute the search\n",
    "    results = collection.aggregate(pipeline)\n",
    "    return list(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835ff0ec",
   "metadata": {},
   "source": [
    "## Step 8: Handling user queries and loading Gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_result(query, collection):\n",
    "\n",
    "    get_knowledge = vector_search(query, collection)\n",
    "\n",
    "    search_result = \"\"\n",
    "    for result in get_knowledge:\n",
    "        search_result += f\"Title: {result.get('title', 'N/A')}, Plot: {result.get('fullplot', 'N/A')}\\n\"\n",
    "\n",
    "    return search_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61d484",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Conduct query with retrival of sources\u001b[39;00m\n\u001b[32m      2\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mWhat is the best romantic movie to watch and why?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m source_information = \u001b[43mget_search_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m combined_information = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mContinue to answer the query by using the Search Results:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msource_information\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(combined_information)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mget_search_result\u001b[39m\u001b[34m(query, collection)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_search_result\u001b[39m(query, collection):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     get_knowledge = \u001b[43mvector_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     search_result = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m get_knowledge:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mvector_search\u001b[39m\u001b[34m(user_query, collection)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mPerform a vector search in the MongoDB collection based on the user query.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03mlist: A list of matching documents.\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Generate embedding for the user query\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m query_embedding = \u001b[43mget_embedding\u001b[49m(user_query)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mInvalid query or embedding generation failed.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'get_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "# Conduct query with retrival of sources\n",
    "query = \"What is the best romantic movie to watch and why?\"\n",
    "source_information = get_search_result(query, collection)\n",
    "combined_information = f\"Query: {query}\\nContinue to answer the query by using the Search Results:\\n{source_information}.\"\n",
    "\n",
    "print(combined_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face token loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not hf_token:\n",
    "    print(\"Hugging Face token (HF_TOKEN) not found in .env file. Please add it.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(\"Hugging Face token loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1b75e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4164e83a93544d9682a2198eeddce1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fd4ca7a8f345878d6313a196292371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  41%|####1     | 2.04G/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "# CPU Enabled uncomment below 👇🏽\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
    "# GPU Enabled use below 👇🏽\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b445c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
    "response = model.generate(**input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(response[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
